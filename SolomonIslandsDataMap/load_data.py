# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_load_data.ipynb.

# %% auto 0
__all__ = ['SolomonGeo']

# %% ../nbs/00_load_data.ipynb 3
from nbdev.showdoc import *
import geopandas as gpd
import pandas as pd
from git import Repo
import json
from fastcore import *
from fastcore.basics import patch
from fastcore.test import *
import sys
import topojson as tp
import pickle

# %% ../nbs/00_load_data.ipynb 6
class SolomonGeo:
    # TODO work out how to format the attributes
    # Look at nbdev docs maybe?
    # TODO change all data to int?
    # TODO - should I make this a dataclass for the auto functionaliy? potentially should try it out
    '''
    Load the solomon islands geography data 
    Attributes:
        geo_df    Geopandas dataframe containing geographies and census data
        geo_levels    A list of the types of available aggregations
        census_vars    A dictionary of census variables in the dataset 
        measure    A dictionary of dictionaries of measures for each census variable in the dataset 
        data_type   Specifies whether the variable is a percentage or number
        locations A dictionary of locations accessed by the geography level
    '''
    def __init__(self, 
                geo_df:gpd.GeoDataFrame): # A geopandas dataset containing population and geography boundaries for each aggregation
        self.geo_df = geo_df

        # variable that tracks the types of aggregations
        self.geo_levels = geo_df.loc[:, ('core', 'agg')].unique()

        # Save a list of census variables, ignoring the core variables
        # Use a dictionary that maps the upper level column names to lower level ones
        var_df = geo_df.drop(columns = "core", level=0)
        vars = {}
        for col in var_df.columns:
            if col[0] not in vars:
                vars[col[0]] = [col[1]]
            else:
                vars[col[0]].append(col[1])
        self.census_vars = vars

        # Measures contains a dictionary of the measure for each census variable, where the
        # key is the seondary name and the value is an array with the primary name and 
        # the secondary appended into a single string. This is necessary to construct a dropdown.
        measure = {}
        for key in vars:
            d2 = {}
            for lab in vars[key]:
                d2[key + ":" + lab] = lab
            measure[key] = d2
            
        self.measure = measure


        # TODO should captialise first letter
        self.data_type = geo_df.loc[:, ('core', 'type')].unique()

        # save a list of locations as a dictionary access by geography level
        locations = {}
        for geo in self.geo_levels:
            locations[geo] = geo_df.loc[geo_df['core']['agg'] == geo].index.unique().sort_values()
        self.locations = locations
        # TODO: need a list of column sub headings: get from column name split by `:`

        self.type_default = 'Total'


    @classmethod
    def read_test(cls,
                 ): # A solmon geo class TODO work out how to return self here... (can't?)
        '''
        Initialise the object using the local testing data
        '''
        # TODO might need to further abstract this concatenation process
        df, geo = cls.extract_from_file('2009')

        gdf = cls.transform('2009', df, geo)

        return cls(
            geo_df = gdf
        )

    @classmethod
    def extract_from_file(cls, 
                            year:str, # The year of that data, only relevant for census data
                 ) -> (pd.DataFrame, 
                      gpd.GeoDataFrame): # Returns input pandas and geopandas datasets
        '''
        Extract and return input datasets from file. Assumes correct format of input dataset
        # TODO specify characteristics of dataset
        # TODO - refact load, too many of the cleaning functions are being done during the loading
        '''
        repo = Repo('.', search_parent_directories=True)
        pw = str(repo.working_tree_dir) + "/testData/"
        df = pd.read_csv(pw + 'sol_census_all_' + year + '.csv')
        aggs = df.loc[:, 'agg'].unique()
        geos = gpd.GeoDataFrame()
        for agg in aggs:
            geo = gpd.read_file(pw + 'sol_geo_' + agg.lower() + '.json')
            # Before combining, need to rename like columns
            # Rename columns and keep only necessary ones, Note that id can be province id, contsituency id etc.
            geo.columns = geo.columns.str.replace(r'^[a-zA-Z]+name$', 'geo_name', case = False, regex = True)
            # TODO this assumes the id key column is the first one (which so far it is...)
            geo.rename(columns = {geo.columns[0]:'id'}, inplace=True)

            # keep only the id and geometry columns
            geo = geo.loc[:, ['id', 'geometry']] 

            # Add an agg column, as the data and geometry need to be joined by id and agg
            geo.loc[:, 'agg'] = agg

            # simplify the geography, use topo to preserver the topology between shapes
            topo = tp.Topology(geo, prequantize=False)
            geo = topo.toposimplify(360/43200).to_gdf()

            geos = pd.concat([geos, geo])

        return (
            df, 
            geos
        )

    @classmethod
    def transform(cls, 
            year:str, # The year of that data, only relevant for census data
            df:pd.DataFrame, # Uncleaned input census dataset
            geo:gpd.GeoDataFrame, # Uncleaned input geospatial dataset
           )-> gpd.GeoDataFrame: # The geopandas dataset for given aggregation
        '''
        Tranform given raw input dataset into a cleaned and combined geopandas dataframe
        '''
        # Clean the geospatial dataframe
        # Add a column that indicates the year
        geo.loc[:, 'year'] = year
        
        # Clean the census data
        df = df.dropna()
        # Rename columns to be consistent across geography
        df.columns = df.columns.str.replace(r'^[a-zA-Z]+_name$', 'geo_name', case = False, regex = True)
        # id needs to change types twice so that it is a string of an int
        df = df.astype({'id': 'int'})#, 'male_pop':'int', 	'female_pop':'int', 'total_pop':'int'})
        df = df.astype({'id': 'str'})
        
        # Merge the data together
        geo_df = geo.merge(df, on=['id', 'agg']).set_index("geo_name") # , 'geo_name'

        # Convert into a multiindex dataframe, with hiearchical columns
        try:
            geo_df = geo_df.rename(columns = {'geometry':'core: geometry', 
                                          'id':'core: id', 'agg':'core: agg', 
                                          'year':'core: year', 'type':'core: type'})
            cols = geo_df.columns.str.extract(r'(.*): (.+)', expand=True)
            geo_df.columns = pd.MultiIndex.from_arrays((cols[0], cols[1]))
            geo_df.columns.names = [None]*2
        except:
            raise ValueError("Issue converting geopandas dataframe to multindex. \
                             Check that all columns have \': \' beside the following\
                             core columns: geometry, id, agg, year, type.")
        # Turn the transformed dataset
        return geo_df

    @classmethod
    def load_pickle(cls,
                    folder:str, #file path of the folder to save in
                    file_name:str = 'sol_geo.pickle' # file name of the saved class
                 ):
        '''
        Initialise the object from a saved filepath
        '''
        # TODO work out how to make this a class method
        repo = Repo('.', search_parent_directories=True)
        pw = str(repo.working_tree_dir) + folder + file_name
        
        with open(pw, 'rb') as f:
            tmp_geo = pickle.load(f)

        # TODO  guide said do below line, don't think relevant though
        #cls.__dict__.update(tmp_dict) 
        
        return cls(
            geo_df = gpd.GeoDataFrame(tmp_geo['geo_df'])
        )
        

# %% ../nbs/00_load_data.ipynb 11
@patch
def save_pickle(self:SolomonGeo,
              folder:str, #file path of the folder to save in
                file_name:str = 'sol_geo.pickle' # file name of the saved class
             ):
    '''
    Save a pickle of the SolomonGeo class
    '''
    repo = Repo('.', search_parent_directories=True)
    pw = str(repo.working_tree_dir) + folder + file_name
    
    f = open(pw, 'wb')
    pickle.dump(self.__dict__, f, 2)
    f.close()


# %% ../nbs/00_load_data.ipynb 15
@patch
def get_geojson(self:SolomonGeo, 
                geo_filter:str = None, # Filters the geojson to the requested aggregation 
               ) -> dict: # Geo JSON formatted dataset
    '''
    A getter method for the SolomonGeo class that returns a Geo JSON formatted dataset
    '''
    ret = self.geo_df
    # Only need geojson from one half of the dataset
    ret = ret.loc[ret['core']['type'] == self.type_default, :]
    if geo_filter is not None:
        ret = ret.loc[ret['core']['agg'] == geo_filter, :]
    # Return only the core data to minimise the html size
    return json.loads(ret.loc[:, ('core', 'geometry')].to_json())

# %% ../nbs/00_load_data.ipynb 17
@patch
def get_df(self:SolomonGeo, 
                geo_filter:str = None, # Filters the dataframe to the requested geography 
                var:str = None, # Selects an upper level 
                measure:str = None, # Selects the lower level variable, if var 1 is used, measure must be used.
                loc_filter:[str] = None, # Filters one of more locations
                # TODO remove hardcoding here?
                type_filter:str = 'Total', # Return either number of proportion
                agg_flag = False, # Whether to return the dataset aggregated for the given selection
               ) -> pd.DataFrame: # Pandas Dataframe containing population data
    '''
    A getter method for the SolomonGeo class that returns a pandas dataset containg
    the id variable and the total population variable. This is the minimal data required
    to display on the map. 
    '''
    ret = self.geo_df
    ret = ret.loc[ret['core']['type'] == type_filter, :]
    # TODO check that filter is valid
    if geo_filter is not None:
        try:
            assert(geo_filter in ['Ward', 'Constituency', 'Province'])
        except:
            ValueError("Geo filter must be one of: ['Ward', 'Constituency', 'Province']")
        ret = ret.loc[ret['core']['agg'] == geo_filter, :]
    # Return only the core data to minimise the html size
    ret = ret.drop(columns = 'core', level=0)

    if loc_filter is not None:
        ret = ret.loc[ret.index.isin(loc_filter), :]

    # Keep only selected column if required
    if measure is not None:
        try:
            assert(var is not None)
            assert(measure in self.census_vars[var])
        except:
            ValueError("If measure is set, var 1 must be set and the key value pair of var and measure must match")
        ret = ret[var].filter(items = [measure])
    elif var is not None:
        # Keep all values from upper level column
        ret = ret[var]
        
    return pd.DataFrame(ret)

# %% ../nbs/00_load_data.ipynb 20
@patch
def agg_df(self:SolomonGeo, 
                geo_filter:str = None, # Filters the dataframe to the requested geography 
                var:str = None, # Selects an upper level 
                measure:str = None, # Selects the lower level variable, if var 1 is used, measure must be used.
                loc_filter:[str] = None, # Filters one of more locations
                # TODO remove hardcoding here?
                type_filter:str = 'Total', # Return either number of proportion
               ) -> pd.Series: # Pandas data series containing aggregated and filtered data
    '''
    A getter method for the SolomonGeo class that calls get_df to get a spcific and then further 
    aggregates that dataset so that the proportion is the weighted proportion
    '''
    df = self.get_df(geo_filter, var, measure, loc_filter, type_filter)

    if type_filter == 'Total':
        df = df.sum()
    elif type_filter == 'Proportion':
        tot_df = self.get_df(geo_filter, var, measure, loc_filter, 'Total')
        df = df * tot_df
        df = df.sum() / tot_df.sum()
    else:
        raise ValueError('The type passed to the aggregate function must be one of the following: \'Total\', \'Proportion\'.')
    return df
